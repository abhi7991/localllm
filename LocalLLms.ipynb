{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b7a9f5",
   "metadata": {},
   "source": [
    "# Localized Large Language Models\n",
    "\n",
    "In this notebook, we explore localized large language models available through **Ollama**, specifically focusing on **Llama3** and **Mistral**. Ollama facilitates running these models on local machines, enabling customizable AI solutions.\n",
    "\n",
    "## Ollama Overview\n",
    "\n",
    "Ollama provides a streamlined interface for running large language models such as **Llama3** and **Mistral** locally. For a detailed guide, you can refer to the [Ollama GitHub page](https://github.com/ollama/ollama).\n",
    "\n",
    "Since I have a windows system I have used the **exe** file which is available on the website. Once installed you can follow the instructions below \n",
    "\n",
    "## Installation\n",
    "\n",
    "For Windows users, you can download and install the **Ollama** application from the official website. Follow these steps to get started:\n",
    "\n",
    "1. **Install Ollama**:\n",
    "   - Download the `.exe` file and install it on your Windows system.\n",
    "2. **Verify Installation**:\n",
    "   - Open a command prompt and type `ollama` to ensure the installation is successful.\n",
    "   - Run [The API](http://localhost:11434/) in a browser and you should see the below output \n",
    "   \n",
    "![alt text](images/Running_1.jpg)         \n",
    "\n",
    "## Using Ollama \n",
    "\n",
    "1. I have used Ollama to run the models using the following 2 methods \n",
    "\n",
    "    - `CURL commands` - I used CURL commands to test out both the models and compare them\n",
    "    - `Command prompt` - This was used to interact with the models and to make the Q&A feature more intuitive and feel less programatic\n",
    "\n",
    "\n",
    "### Steps to run Llama 3\n",
    "    - Pulling the Llama3 source code locally\n",
    "    - Lets see the available environments \n",
    "![alt text](images/Running_Ollama1.jpg)\n",
    "\n",
    "    - Lets get Llama3 up and running \n",
    "    - Once we run Llama3 we can test it out with some examples \n",
    "\n",
    "![alt text](images/Running_Ollama2.jpg)\n",
    "\n",
    "    - We can also run custom Llama3 models and prompt it to do different things \n",
    "    - Here is an example I tried out using the modelfile present in the repository\n",
    "    - running the below code creates a new model\n",
    "    \n",
    " ```ollama create myllama3 -f myllama3.modelfile```\n",
    "\n",
    "![alt text](images/Running_Custom_Ollama3.jpg)    \n",
    "    \n",
    "    - Testing out the new model we can see our custom AI bot has been deployed \n",
    "    \n",
    "    \n",
    "![alt text](images/Running_Custom_Ollama4.jpg)        \n",
    "\n",
    "\n",
    "### Steps to run Mistral\n",
    "\n",
    "    - Similar to running Llama3 we can run Mistral by installing it Through Ollama\n",
    "    - When we list out the models we can see that our Mistral model is available to us \n",
    "    \n",
    "![alt text](images/Running_Custom_mistral5.jpg)           \n",
    "\n",
    "    - Testing out the Model\n",
    "    \n",
    "![alt text](images/Running_Custom_mistral5_5.jpg)           \n",
    "\n",
    "    - Similar to our Custom LLama3 model we create a custom Mistral model to see how it performs\n",
    "     - Testing out the new model we can see our custom Mistral AI bot has been deployed     \n",
    "![alt text](images/Running_Custom_mistral6.jpg)               \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6776b",
   "metadata": {},
   "source": [
    "## Comparing the Large Language Models\n",
    "\n",
    "We can compare the large language models by passing using the Curl command and obtaining important information \n",
    "\n",
    "```\n",
    "curl -X POST http://localhost:11434/api/generate -d \"{\\\"model\\\": \\\"llama3\\\",  \\\"prompt\\\":\\\"Tell me a fact about Llama?\\\", \\\"stream\\\": false}\"\n",
    "```\n",
    "we get the following parameters upon running the API \n",
    "\n",
    "- `total_duration`: time spent generating the response\n",
    "- `load_duration`: time spent in nanoseconds loading the model\n",
    "- `prompt_eval_count`: number of tokens in the prompt\n",
    "- `prompt_eval_duration`: time spent in nanoseconds evaluating the prompt\n",
    "- `eval_count`: number of tokens in the response\n",
    "- `eval_duration`: time in nanoseconds spent generating the response\n",
    "- `context`: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory\n",
    "- `response`: empty if the response was streamed, if not streamed, this will contain the full response\n",
    "\n",
    "Now I have created a custom script to run a set of questions and compare some of the parameters across the models\n",
    "\n",
    "here are some of the outputs I got when comparing the following parameters \n",
    "\n",
    "- `prompt_eval_duration`: time spent in nanoseconds evaluating the prompt\n",
    "- `eval_duration`: time in nanoseconds spent generating the response\n",
    "\n",
    "           \n",
    "\n",
    "![alt text](images/Prompt_Eval_Duration_.jpg)             \n",
    "\n",
    "\n",
    "![alt text](images/Prompt_Eval_Duration_.jpg)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c928ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jun 16 15:06:01 2024\n",
    "\n",
    "@author: abhis\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup'\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "This function is used to run a particular Llama model and generate the metrics\n",
    "''' \n",
    "def getInsights(model, question):\n",
    "    url = 'http://localhost:11434/api/generate'\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": question,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    data_json = json.dumps(data)\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(url, data=data_json, headers={'Content-Type': 'application/json'})\n",
    "        r.raise_for_status() \n",
    "        df = r.json() \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n",
    "'''\n",
    "This function plots the metrics obtained upon running each question\n",
    "''' \n",
    "def plot_metrics(df, metric):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df, x=\"Question\", y=metric, hue=\"Model\", marker=\"o\")\n",
    "    plt.title(f\"Comparison of {metric} Across Questions\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel(\"Question\")\n",
    "    plt.ylabel(f\"{metric} (s)\")\n",
    "    plt.legend(title=\"Model\")\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.getcwd()+\"/\"+metric+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Converting nanoseconds to seconds for better understanding\n",
    "'''\n",
    "def nanoseconds_to_seconds(nanoseconds):\n",
    "    return nanoseconds / 1_000_000_000\n",
    "\n",
    "questions = [\n",
    "    \"Who are you and what is the capital of India?\",\n",
    "    \"What is the largest mammal in the world?\",\n",
    "    \"How do you make an apple pie?\",\n",
    "    \"What are the benefits of exercise?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "]\n",
    "\n",
    "metrics_data = {\n",
    "    \"Model\": [],\n",
    "    \"Question\": [],\n",
    "    \"Eval Duration (s)\": [],\n",
    "    \"Prompt Eval Duration (s)\": []\n",
    "}\n",
    "\n",
    "'''\n",
    "Collect metrics for both models\n",
    "\n",
    "mymistral - This is the custom Mistral model I made using the modelfile \n",
    "\n",
    "myllama3 - This is the custom Llama3 model I made using the modelfile \n",
    "\n",
    "'''\n",
    "for model in [\"mymistral\", \"myllama3\"]:\n",
    "    for question in questions:\n",
    "        response = getInsights(model, question)\n",
    "        if response:\n",
    "            metrics_data[\"Model\"].append(model)\n",
    "            metrics_data[\"Question\"].append(question)\n",
    "            metrics_data[\"Eval Duration (s)\"].append(nanoseconds_to_seconds(response['eval_duration']))\n",
    "            metrics_data[\"Prompt Eval Duration (s)\"].append(nanoseconds_to_seconds(response['prompt_eval_duration']))\n",
    "\n",
    "a = metrics_data\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "print(df_metrics)\n",
    "\n",
    "# Plot each metric\n",
    "for metric in [\"Eval Duration (s)\", \"Prompt Eval Duration (s)\"]:\n",
    "    plot_metrics(df_metrics, metric)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
