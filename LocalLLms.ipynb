{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7fc4d6",
   "metadata": {},
   "source": [
    "# Localized Large Language Models\n",
    "\n",
    "In this notebook, we explore localized large language models available through **Ollama**, specifically focusing on **Llama3** and **Mistral**. Ollama facilitates running these models on local machines, enabling customizable AI solutions.\n",
    "\n",
    "## Ollama Overview\n",
    "\n",
    "Ollama provides a streamlined interface for running large language models such as **Llama3** and **Mistral** locally. For a detailed guide, you can refer to the [Ollama GitHub page](https://github.com/ollama/ollama).\n",
    "\n",
    "Since I have a windows system I have used the **exe** file which is available on the website. Once installed you can follow the instructions below \n",
    "\n",
    "### Installation\n",
    "\n",
    "For Windows users, you can download and install the **Ollama** application from the official website. Follow these steps to get started:\n",
    "\n",
    "1. **Install Ollama**:\n",
    "   - Download the `.exe` file and install it on your Windows system.\n",
    "2. **Verify Installation**:\n",
    "   - Open a command prompt and type `ollama` to ensure the installation is successful.\n",
    "   - Run [The API](http://localhost:11434/) in a browser and you should see the below output \n",
    "   \n",
    "![alt text](images/Running_1.jpg)           \n",
    "\n",
    "### Running Llama3\n",
    "    - Pulling the Llama3 source code locally\n",
    "    - Lets see the available environments \n",
    "![alt text](images/Running_Ollama1.jpg)\n",
    "\n",
    "    - Lets get Llama3 up and running \n",
    "    - Once we run Llama3 we can test it out with some examples \n",
    "\n",
    "![alt text](images/Running_Ollama2.jpg)\n",
    "\n",
    "    - We can also run custom Llama3 models and prompt it to do different things \n",
    "    - Here is an example I tried out using the modelfile present in the repository\n",
    "    - running the below code creates a new model\n",
    "    \n",
    " ```ollama create myllama3 -f myllama3.modelfile```\n",
    "\n",
    "![alt text](images/Running_Custom_Ollama3.jpg)    \n",
    "    \n",
    "    - Testing out the new model we can see our custom AI bot has been deployed \n",
    "    \n",
    "    \n",
    "![alt text](images/Running_Custom_Ollama4.jpg)        \n",
    "\n",
    "\n",
    "### Running Mistral\n",
    "\n",
    "    - Similar to running Llama3 we can run Mistral by installing it Through Ollama\n",
    "    - When we list out the models we can see that our Mistral model is available to us \n",
    "    \n",
    "![alt text](images/Running_Custom_mistral5.jpg)           \n",
    "\n",
    "    - Testing out the Model\n",
    "    \n",
    "![alt text](images/Running_Custom_mistral5_5.jpg)           \n",
    "\n",
    "    - Similar to our Custom LLama3 model we create a custom Mistral model to see how it performs\n",
    "     - Testing out the new model we can see our custom Mistral AI bot has been deployed     \n",
    "![alt text](images/Running_Custom_mistral6.jpg)               \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0aa3e",
   "metadata": {},
   "source": [
    "# Comparing the Large Language Models\n",
    "- We can compare the large language models by passing a set of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
